{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Scraper \n",
    "\n",
    "python modules\n",
    "* diffÃ©rents modules python  tweepy, twint, twitterscraper https://larevueia.fr/nlp-avec-python-analyse-de-sentiments-sur-twitter/\n",
    "* credentials pour utiliser l'API de twitter https://developer.twitter.com/en/portal/apps/8475429/keys\n",
    "* module python pour interfacer l'API tweepy api https://docs.tweepy.org/en/latest/api.html ; https://docs.tweepy.org/en/latest/api.html#API.search ; https://docs.tweepy.org/en/latest/cursor_tutorial.html?highlight=cursor#cursor-tutorial\n",
    "* https://github.com/JustAnotherArchivist/snscrape\n",
    "\n",
    "Twitter et TAL\n",
    "* prÃ©traiter des textes issus de twitter https://github.com/cbaziotis/ekphrasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## twitterscraper \n",
    "\n",
    "api non fonctionnelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twitterscraper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-07d2f2d80d0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwitterscraper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquery_tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdebut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2021\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'twitterscraper'"
     ]
    }
   ],
   "source": [
    "from twitterscraper import query_tweets\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "debut = dt.date(2021,1,1)\n",
    "fin = dt.date(2021,12,31)\n",
    "#mots=\"Covid-19 OR Covid OR Corona OR PandÃ©mie OR Ã©pidÃ©mie OR Coronavirus OR virus\"\n",
    "mots = \"ğŸ˜ \"\n",
    "mots = \"colÃ¨re\"\n",
    "\n",
    "tweets = query_tweets(query=mots, begindate = debut, enddate = fin, lang = \"fr\")\n",
    "\n",
    "tweets = pd.DataFrame(t.__dict__ for t in tweets)\n",
    "\n",
    "tweets.to_csv('built/tweet_angry.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  tweepy\n",
    "\n",
    "`sudo pip3 install tweepy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Require a developer twitter account to obtain API/Consumer key and secret as well as access token key and secret. \n",
    "* Require a premium access to go further than the 7-day limit of the search index.\n",
    "\n",
    "See https://developer.twitter.com/en/portal/apps/8475429/keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b48dc50f89ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconsumer_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SET_ME\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconsumer_secret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SET_ME\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "\n",
    "consumer_key = \"SET_ME\" \n",
    "consumer_secret = \"SET_ME\"\n",
    "access_token_key = \"SET_ME\"\n",
    "access_token_secret = \"SET_ME\"\n",
    "\n",
    "# Authentification :\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token_key, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "requete = \"ğŸ˜ \"# \"Covid-19 OR Covid OR Corona OR PandÃ©mie OR Ã©pidÃ©mie OR Coronavirus OR virus\"\n",
    "\n",
    "tweets = tw.Cursor(api.search,\n",
    "                   q = requete,\n",
    "                   lang = \"fr\",\n",
    "                   since='2018-01-01').items(1000)\n",
    "\n",
    "all_tweets = [tweet.text for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [tweet.entities for tweet in tweets]\n",
    "print (entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â twint\n",
    "\n",
    "* main page https://github.com/twintproject/twint\n",
    "* documentation: configuration options, tweets attributes... https://github.com/twintproject/twint/wiki/Configuration\n",
    "* tutoriel avec api https://towardsdatascience.com/analyzing-tweets-with-nlp-in-minutes-with-spark-optimus-and-twint-a0c96084995f\n",
    "\n",
    "  `sudo pip install twint`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How to use twint ?\n",
    "\n",
    "#### .Limit, .Since .Until options\n",
    "\n",
    "Twint is said to be slow but this is not the main issue.\n",
    "\n",
    "From the [Twint API](https://github.com/twintproject/twint/wiki/Configuration), we can read that Twint allows to\n",
    "* set a limit of number of Tweets to pull. \n",
    "* filter Tweets sent since date, works only with twint.run.Search (Example: 2017-12-27).\n",
    "* filter Tweets sent until date, works only with twint.run.Search (Example: 2017-12-27).\n",
    "\n",
    "When successive Search requests are run, if the limit of tweets to pull is too low or the period is too short, you may face the `CRITICAL:root:twint.run:Twint:Feed:noData'globalObjects'` issue. The issue stops the collect process. It seems to occur when the time between successive Search requests is repeatedly too short.\n",
    "\n",
    "Setting a limit too high on a short period may be not the solution when there are not so many tweets to fetch (i.e. `[!] No more data! Scraping will stop now.`). \n",
    "\n",
    "I recommend to test and determine how many tweets can be fetched on a certain time slice and consequently to conclude what is the best time slice for your resquest to fetch. \n",
    "\n",
    "For example, when setting a limit of 1000 with a period of 1 day (and repeating the search on several successive days), you obtain 100 hundreds tweets for the whole period, then you may set a period of 10 * 1 days (or a bit more).\n",
    "\n",
    "#### Search request options\n",
    "\n",
    "various kind of resquests:\n",
    "``` \n",
    "request = \"ğŸ˜  OR ğŸ˜¡ OR ğŸ˜¤\"\n",
    "request = \"ğŸ˜  AND ğŸ˜¡ AND ğŸ˜¤\" \n",
    "request = \"Banksy\" \n",
    "request = \"Un Banksy vendu plus de 19 millions dâ€™euros, une somme qui sera reversÃ©e au service de santÃ© britannique\" \n",
    "``` \n",
    "\n",
    "#### How to focus on a specific language ? \n",
    "\n",
    "* `tw.lang='fr'`\n",
    "doesnt not prevent to retrieve tweets in other language. The tweet lang attribute helps to filter after fetching.\n",
    "\n",
    "* `tw.Geo = \"46.6055983, 1.8750922, 545km\" `                  (string) - Geo coordinates (lat,lon,km/mi.) ; \n",
    "https://fr.wikipedia.org/wiki/Centre_de_la_France # 543,7 km ;  Centre du cercle minimum \n",
    "Not convinced by the effect of the distance specification.\n",
    "\n",
    "* `tw.Near = \"Paris\"`\n",
    "Seems work pretty well. But introduce a bias. \"Paris\" is one of the tweet's topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â `CRITICAL:root:twint.run:Twint:Feed:noDataExpecting value: line 1 column 1 (char 0)`\n",
    "\n",
    "https://github.com/twintproject/twint/issues/1063\n",
    "\n",
    "```\n",
    "sudo pip3 uninstall twint\n",
    "sudo pip3 install --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â `RuntimeError: This event loop is already running`\n",
    "\n",
    "Solve compatibility issues with notebooks and RunTime errors.\n",
    "\n",
    "```\n",
    "sudo pip3 install nest_asyncio\n",
    "```\n",
    "\n",
    "Then in python code:\n",
    "```\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `CRITICAL:root:twint.run:Twint:Feed:noData'globalObjects'`\n",
    "\n",
    "When too many requests with no pause between them. A pause can be created simply by increasing the limit of the tweets to fetch. \n",
    "\n",
    "Approximatively we note that a limit set to 1000 last 10 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching some tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package\n",
    "import twint\n",
    "import nest_asyncio\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import calendar\n",
    "import os\n",
    "\n",
    "def add_time_delta(a_date):\n",
    "    #a_date += datetime.timedelta(minutes=15)\n",
    "    #a_date += datetime.timedelta(days=1)\n",
    "    a_date += datetime.timedelta(weeks=1)\n",
    "    return a_date\n",
    "#\n",
    "def add_months(sourcedate, months):\n",
    "    month = sourcedate.month - 1 + months\n",
    "    year = sourcedate.year + month // 12\n",
    "    month = month % 12 + 1\n",
    "    day = min(sourcedate.day, calendar.monthrange(year,month)[1])\n",
    "    return datetime.datetime(year, month, day,sourcedate.hour, sourcedate.minute, sourcedate.second)\n",
    "\n",
    "# emotions\n",
    "anger = \"ğŸ˜£ OR ğŸ˜© OR ğŸ˜« OR ğŸ’” OR ğŸ˜ OR â€¼ OR ğŸ‘Š OR ğŸ”« OR ğŸ’¥ OR â— OR ğŸ”¥ OR ğŸ™… OR ğŸ‘ OR âŒ OR ğŸ˜’ OR ğŸ˜¤ OR ğŸ˜¡ OR ğŸ˜ \"\n",
    "anticipation = \"ğŸ€ OR ğŸ˜‰ OR âœŒ OR ğŸ˜» OR ğŸ˜Š OR â˜º OR ğŸ˜œ OR ğŸ˜• OR ğŸ˜± OR â€¼ OR â— OR ğŸ» OR ğŸ˜š OR âœˆ OR ğŸ™ OR ğŸ™Š OR ğŸ˜… OR ğŸ˜ˆ OR ğŸ˜“ OR ğŸ˜Ÿ OR ğŸ˜‹ OR ğŸ˜™ OR ğŸ’“ OR âœŠ OR ğŸ™‹ OR ğŸ˜¬ OR ğŸ’° OR ğŸ’­ OR ğŸ‘€\"\n",
    "disgust = \"ğŸ’” OR ğŸ˜¶ OR ğŸ˜¨ OR ğŸ˜³ OR ğŸ˜­ OR ğŸ˜¢ OR ğŸ˜• OR ğŸ˜° OR ğŸ˜± OR ğŸ˜ª OR ğŸ˜Ÿ OR ğŸ˜‘ OR ğŸ˜ OR ğŸ˜© OR ğŸ˜· OR ğŸ™… OR ğŸ˜¡ OR ğŸ˜  OR âŒ OR ğŸ˜¤ OR ğŸ˜« OR ğŸ˜’ OR ğŸ’© OR ğŸ˜£ OR ğŸ˜– OR ğŸ‘\"\n",
    "fear = \"ğŸ˜ OR ğŸ˜­ OR ğŸ˜¥ OR ğŸ™… OR ğŸ˜¶ OR ğŸ˜• OR ğŸ‘€ OR â— OR ğŸ˜¬ OR ğŸ˜£ OR ğŸ˜© OR ğŸ™Š OR ğŸ˜· OR ğŸ˜¢ OR ğŸ˜– OR ğŸ˜³ OR ğŸ’€ OR ğŸ˜“ OR ğŸ‘» OR ğŸ˜Ÿ OR ğŸ˜° OR ğŸ˜± OR ğŸ˜¨\"\n",
    "joy = \"ğŸ’˜ OR ğŸ’ƒ OR ğŸ˜› OR ğŸ˜€ OR ğŸŒˆ OR ğŸ’ OR â¤ OR ğŸ™Œ OR ğŸ˜‡ OR ğŸ» OR â™¥ OR ğŸ˜˜ OR ğŸ˜ OR ğŸ˜» OR ğŸ˜‹ OR ğŸŒ OR ğŸ’‹ OR ğŸ˜Œ OR ğŸ˜ OR ğŸ˜ƒ OR ğŸ˜ OR ğŸ’ OR ğŸ’• OR ğŸ˜™ OR ğŸ˜„ OR ğŸ˜š OR ğŸ˜ OR ğŸ’— OR ğŸ’– OR ğŸ‰ OR ğŸ˜Š OR ğŸ˜‚ OR ğŸ˜¹ OR ğŸ˜† OR â˜º\"\n",
    "sadeness = \"ğŸ˜“ OR ğŸ˜¡ OR ğŸ˜• OR ğŸ˜’ OR ğŸ˜– OR ğŸ˜¨ OR ğŸ˜ª OR ğŸ˜£ OR ğŸ˜° OR ğŸ˜Ÿ OR ğŸ˜” OR ğŸ˜« OR ğŸ˜© OR ğŸ˜¥ OR ğŸ˜ OR ğŸ’” OR ğŸ˜­ OR ğŸ˜¢\" # 15 minutes sur Paris\n",
    "surprise = \"âš¡ OR ğŸ’ OR ğŸ€ OR ğŸ˜… OR ğŸ”¥ OR ğŸ™Œ OR ğŸ˜¨ OR â˜º OR ğŸ™Š OR âœ¨ OR ğŸ˜³ OR ğŸ˜ OR ğŸ‰ OR ğŸ™ˆ OR ğŸ‘€ OR ğŸ˜± OR â— OR â€¼\"\n",
    "trust = \"ğŸ˜Š OR ğŸ™Œ OR â˜‘ OR âœŒ OR ğŸ‘ OR ğŸ˜‰ OR ğŸ’ OR ğŸ’— OR ğŸ˜‡ OR ğŸ˜ OR â˜º OR ğŸ» OR ğŸ’– OR ğŸ‘ OR ğŸ’¯ OR ğŸ‘Œ OR â™¥ OR ğŸ’ OR ğŸ™ OR ğŸ’ OR ğŸ˜Œ OR ğŸ˜˜ OR ğŸ™† OR ğŸ‘Š OR ğŸ’˜ OR ğŸ’“ OR ğŸ˜š OR â¤ OR ğŸ’‹ OR ğŸŒ¹ OR ğŸ’• OR ğŸ˜™\"\n",
    "emotional_emojis_dict = dict()\n",
    "emotional_emojis_dict['anger'] = anger\n",
    "emotional_emojis_dict['anticipation'] = anticipation\n",
    "emotional_emojis_dict['disgust'] = disgust\n",
    "emotional_emojis_dict['fear'] = fear\n",
    "emotional_emojis_dict['joy'] = joy\n",
    "emotional_emojis_dict['sadeness'] = sadeness\n",
    "emotional_emojis_dict['surprise'] = surprise\n",
    "emotional_emojis_dict['trust'] = trust\n",
    "\n",
    "# twint\n",
    "nest_asyncio.apply()\n",
    "\n",
    "tw = twint.Config()\n",
    "tw.Pandas = True\n",
    "tw.Store_pandas = True\n",
    "tw.Pandas_clean = True\n",
    "tw.Hide_output = True\n",
    "\n",
    "\n",
    "plus_grandes_villes_de_france = ['Paris', 'Marseille', 'Lyon', 'Toulouse', 'Nice', 'Nantes', 'Montpellier', 'Strasbourg', 'Bordeaux', 'Lille', 'Rennes', 'Reims', 'Saint-Ã‰tienne', 'Le Havre', 'Toulon', 'Grenoble', 'Dijon', 'Angers', 'NÃ®mes', 'Villeurbanne']\n",
    "# 'Monaco', 'Bruxelles', 'QuÃ©bec',  'London', 'Dublin', 'Madrid', 'Lisbone', 'Berlin', 'Rome', 'Vienne', 'Luxembourg', \n",
    "len(plus_grandes_villes_de_france)\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "\n",
    "# emotional_tweets_dict = dict()\n",
    "\n",
    "# for each time delta\n",
    "def fetch_tweets(start_date, end_date, add_time_delta, plus_grandes_villes_de_france, outputDir):\n",
    "    next_start_date = start_date\n",
    "    \n",
    "    tmp_start_date = start_date\n",
    "    tmp_next_start_date = next_start_date\n",
    "    total_delta_counter = 0\n",
    "    while tmp_start_date < end_date:\n",
    "        tmp_next_start_date = add_time_delta(tmp_next_start_date)\n",
    "        tmp_start_date = tmp_next_start_date\n",
    "        total_delta_counter += 1\n",
    "    print (\"INFO: time delta factor = \"+str(total_delta_counter))\n",
    "\n",
    "    \n",
    "    print (\"INFO: start_date: \"+str(start_date))\n",
    "    \n",
    "    scraping_start = time.time()\n",
    "    print (\"INFO: scraping Twitter starting at: \"+str(scraping_start)+\"...\")\n",
    "    \n",
    "    delta_counter = 0\n",
    "    while start_date < end_date:\n",
    "        next_start_date = add_time_delta(next_start_date)\n",
    "    \n",
    "        day_start = time.time()\n",
    "\n",
    "        # for each top city\n",
    "        city_counter = 0\n",
    "        # SET \n",
    "        #for v in plus_grandes_villes_de_france:\n",
    "        for v in plus_grandes_villes_de_france:\n",
    "            city_counter += 1\n",
    "            tw.Near = v\n",
    "            city_start = time.time()\n",
    "\n",
    "            # for each affect\n",
    "            emotion_counter = 0\n",
    "            for e in emotional_emojis_dict:\n",
    "                #print ('Debug: e\\t',e)\n",
    "                emotion_counter += 1\n",
    "                affect_start = time.time()\n",
    "\n",
    "                tw.Search =  emotional_emojis_dict[e]\n",
    "\n",
    "                #\n",
    "                tw.Since = str(start_date)\n",
    "                tw.Until = str(next_start_date)\n",
    "            \n",
    "                #\n",
    "                twint.run.Search(tw)\n",
    "                #if e not in emotional_tweets_dict:\n",
    "                #    emotional_tweets_dict[e] = twint.storage.panda.Tweets_df\n",
    "                #else:\n",
    "                #    emotional_tweets_dict[e] = emotional_tweets_dict[e].append(twint.storage.panda.Tweets_df, ignore_index=True)\n",
    "                current_d_v_e_df = twint.storage.panda.Tweets_df\n",
    "                affect_end = time.time()\n",
    "                outputfile = outputDir+\"/tweets_\"+e+'_'+v+'_'+str(start_date).replace(' ','_')+'_'+str(next_start_date).replace(' ','_')+'.csv' #'+.replace(' ','_')\n",
    "                print (\"INFO: \"+str(delta_counter)+\"/\"+str(total_delta_counter)+\"\\t\"+str(start_date).replace(' ','_')+\"\\t\"+v+\"(\"+str(city_counter)+\"/\"+str(len(plus_grandes_villes_de_france))+\")\\t\"+e+\"(\"+str(emotion_counter)+\"/\"+str(len(emotional_emojis_dict))+\")\\t\"+str(affect_end - affect_start))\n",
    "                if not (current_d_v_e_df.empty):\n",
    "                    current_d_v_e_df['emotion'] = [e]*len(current_d_v_e_df)\n",
    "                    current_d_v_e_df['city'] = [v]*len(current_d_v_e_df)\n",
    "                    current_d_v_e_df['since'] = [tw.Since.split(' ')[0]]*len(current_d_v_e_df)\n",
    "                \n",
    "                #print(\"INFO: scraping affect \"+e+\" ellapsed time: \"+str(affect_end - affect_start))\n",
    "                    print (\"INFO: Printing \"+outputfile) #+\" ; ellapsed time: \"+str(affect_end - affect_start))\n",
    "                #emotional_tweets_dict[e]\n",
    "                    current_d_v_e_df.to_csv(outputfile, sep = '\\t', index=False) \n",
    "            #else: \n",
    "\n",
    "            city_end = time.time()\n",
    "        #print(\"INFO: scraping city \"+v+\" ellapsed time: \"+str(city_end - city_start))\n",
    "    \n",
    "        day_end = time.time()\n",
    "    #print(\"INFO: scraping dat \"+str(start_date)+\" ellapsed time: \"+str(day_end - day_start))\n",
    "    \n",
    "        start_date = next_start_date\n",
    "        delta_counter += 1\n",
    "        print ()\n",
    "            \n",
    "    scraping_end = time.time()\n",
    "    print(\"INFO: scraping ellapsed time: \"+str(scraping_end - scraping_start))\n",
    "       #tweets_df = pd.DataFrame()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME how the data_path was populated...\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Setting 0\n",
    "start_date = datetime.datetime(2018,1,1,0,0,0)\n",
    "end_date = datetime.datetime(2018,1,8,0,0,0)\n",
    "\n",
    "plus_grandes_villes_de_france = ['Paris', 'Marseille', 'Lyon', 'Toulouse', 'Nice', 'Nantes', 'Montpellier', 'Strasbourg', 'Bordeaux', 'Lille', 'Rennes', 'Reims', 'Saint-Ã‰tienne', 'Le Havre', 'Toulon', 'Grenoble', 'Dijon', 'Angers', 'NÃ®mes', 'Villeurbanne']\n",
    "\n",
    "def add_time_delta(a_date):\n",
    "    a_date += datetime.timedelta(days=1)\n",
    "    return a_date\n",
    "\n",
    "# \n",
    "outputDir = \"built/100tweets_top20-city-fr_8emo_20180101-0107\"\n",
    "outputDir = \"/tmp/100tweets_top20-city-fr_8emo_20180101-0107\"\n",
    "\n",
    "if not(os.path.isfile(outputDir+\".zip\")) and not(os.path.isdir(outputDir)):\n",
    "    os.mkdir(outputDir)\n",
    "\n",
    "tw.Limit = 100 # 5000 ~ 60 # 1000 ~10s (sans GÃ©o)\n",
    "tw.Lang = \"fr\"\n",
    "\n",
    "# UNcomment to run\n",
    "fetch_tweets(start_date, end_date, add_time_delta, plus_grandes_villes_de_france, outputDir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison de l'option native de stockage de twing et celle ultÃ©rieure via dataframe\n",
    "\n",
    "AprÃ¨s observations des deux fichiers produits. Avec dataframe, on peut choisir le sÃ©parateur et rajouter des colonnes. Il n'y a pas de perte d'information dans aucun des deux cas. Le champ tweet est le mÃªme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "# twint\n",
    "nest_asyncio.apply()\n",
    "\n",
    "tw = twint.Config()\n",
    "tw.Pandas = True\n",
    "tw.Store_pandas = True\n",
    "tw.Pandas_clean = True\n",
    "tw.Hide_output = True\n",
    "\n",
    "c = \"Paris\"\n",
    "e = \"CafÃ©\"\n",
    "tw.Limit = 1000 # 1000 ~10s (sans GÃ©o)\n",
    "tw.Since = str(datetime.datetime(2018,1,1,0,0,0))\n",
    "tw.Until = str(datetime.datetime(2018,1,10,0,0,0))\n",
    "tw.Near = c\n",
    "tw.Search = e\n",
    "tw.Store_pandas = True\n",
    "outputName = \"/tmp/tweets_\"+e+'_'+c+'_'+tw.Since.replace(' ','_')+'_'+tw.Until .replace(' ','_') #+'.csv' #'+\n",
    "tw.Output = outputName+\"_tw.csv\"        \n",
    "twint.run.Search(tw)\n",
    "current_d_v_e_df = twint.storage.panda.Tweets_df\n",
    "current_d_v_e_df['emotion'] = [e]*len(current_d_v_e_df)\n",
    "current_d_v_e_df['city'] = [c]*len(current_d_v_e_df)\n",
    "current_d_v_e_df['since'] = [tw.Since.split(' ')[0]]*len(current_d_v_e_df)\n",
    "print (len(current_d_v_e_df))\n",
    "# current_d_v_e_df.head()\n",
    "# current_d_v_e_df['tweet'][0]\n",
    "current_d_v_e_df.to_csv(outputName+\"_pd.csv\", sep = '\\t', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo pip3 install langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "for t in tweet:\n",
    "    print (t['tweet'])\n",
    "    #if detect(t['tweet']) == 'fr': \n",
    "    #    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
