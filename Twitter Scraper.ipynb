{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Scraper \n",
    "\n",
    "python modules\n",
    "* différents modules python  tweepy, twint, twitterscraper https://larevueia.fr/nlp-avec-python-analyse-de-sentiments-sur-twitter/\n",
    "* credentials pour utiliser l'API de twitter https://developer.twitter.com/en/portal/apps/8475429/keys\n",
    "* module python pour interfacer l'API tweepy api https://docs.tweepy.org/en/latest/api.html ; https://docs.tweepy.org/en/latest/api.html#API.search ; https://docs.tweepy.org/en/latest/cursor_tutorial.html?highlight=cursor#cursor-tutorial\n",
    "* https://github.com/JustAnotherArchivist/snscrape\n",
    "\n",
    "Twitter et TAL\n",
    "* prétraiter des textes issus de twitter https://github.com/cbaziotis/ekphrasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## twitterscraper \n",
    "\n",
    "api non fonctionnelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twitterscraper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-07d2f2d80d0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwitterscraper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquery_tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdebut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2021\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'twitterscraper'"
     ]
    }
   ],
   "source": [
    "from twitterscraper import query_tweets\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "debut = dt.date(2021,1,1)\n",
    "fin = dt.date(2021,12,31)\n",
    "#mots=\"Covid-19 OR Covid OR Corona OR Pandémie OR épidémie OR Coronavirus OR virus\"\n",
    "mots = \"😠\"\n",
    "mots = \"colère\"\n",
    "\n",
    "tweets = query_tweets(query=mots, begindate = debut, enddate = fin, lang = \"fr\")\n",
    "\n",
    "tweets = pd.DataFrame(t.__dict__ for t in tweets)\n",
    "\n",
    "tweets.to_csv('built/tweet_angry.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  tweepy\n",
    "\n",
    "`sudo pip3 install tweepy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Require a developer twitter account to obtain API/Consumer key and secret as well as access token key and secret. \n",
    "* Require a premium access to go further than the 7-day limit of the search index.\n",
    "\n",
    "See https://developer.twitter.com/en/portal/apps/8475429/keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b48dc50f89ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconsumer_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SET_ME\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconsumer_secret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SET_ME\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "\n",
    "consumer_key = \"SET_ME\" \n",
    "consumer_secret = \"SET_ME\"\n",
    "access_token_key = \"SET_ME\"\n",
    "access_token_secret = \"SET_ME\"\n",
    "\n",
    "# Authentification :\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token_key, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "requete = \"😠\"# \"Covid-19 OR Covid OR Corona OR Pandémie OR épidémie OR Coronavirus OR virus\"\n",
    "\n",
    "tweets = tw.Cursor(api.search,\n",
    "                   q = requete,\n",
    "                   lang = \"fr\",\n",
    "                   since='2018-01-01').items(1000)\n",
    "\n",
    "all_tweets = [tweet.text for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [tweet.entities for tweet in tweets]\n",
    "print (entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## twint\n",
    "\n",
    "* main page https://github.com/twintproject/twint\n",
    "* documentation: configuration options, tweets attributes... https://github.com/twintproject/twint/wiki/Configuration\n",
    "* tutoriel avec api https://towardsdatascience.com/analyzing-tweets-with-nlp-in-minutes-with-spark-optimus-and-twint-a0c96084995f\n",
    "\n",
    "  `sudo pip install twint`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How to use twint ?\n",
    "\n",
    "#### .Limit, .Since .Until options\n",
    "\n",
    "Twint is said to be slow but this is not the main issue.\n",
    "\n",
    "From the [Twint API](https://github.com/twintproject/twint/wiki/Configuration), we can read that Twint allows to\n",
    "* set a limit of number of Tweets to pull. \n",
    "* filter Tweets sent since date, works only with twint.run.Search (Example: 2017-12-27).\n",
    "* filter Tweets sent until date, works only with twint.run.Search (Example: 2017-12-27).\n",
    "\n",
    "When successive Search requests are run, if the limit of tweets to pull is too low or the period is too short, you may face the `CRITICAL:root:twint.run:Twint:Feed:noData'globalObjects'` issue. The issue stops the collect process. It seems to occur when the time between successive Search requests is repeatedly too short.\n",
    "\n",
    "Setting a limit too high on a short period may be not the solution when there are not so many tweets to fetch (i.e. `[!] No more data! Scraping will stop now.`). \n",
    "\n",
    "I recommend to test and determine how many tweets can be fetched on a certain time slice and consequently to conclude what is the best time slice for your resquest to fetch. \n",
    "\n",
    "For example, when setting a limit of 1000 with a period of 1 day (and repeating the search on several successive days), you obtain 100 hundreds tweets for the whole period, then you may set a period of 10 * 1 days (or a bit more).\n",
    "\n",
    "#### Search request options\n",
    "\n",
    "various kind of resquests:\n",
    "``` \n",
    "request = \"😠 OR 😡 OR 😤\"\n",
    "request = \"😠 AND 😡 AND 😤\" \n",
    "request = \"Banksy\" \n",
    "request = \"Un Banksy vendu plus de 19 millions d’euros, une somme qui sera reversée au service de santé britannique\" \n",
    "``` \n",
    "\n",
    "#### How to focus on a specific language ? \n",
    "\n",
    "* `tw.lang='fr'`\n",
    "doesnt not prevent to retrieve tweets in other language. The tweet lang attribute helps to filter after fetching.\n",
    "\n",
    "* `tw.Geo = \"46.6055983, 1.8750922, 545km\" `                  (string) - Geo coordinates (lat,lon,km/mi.) ; \n",
    "https://fr.wikipedia.org/wiki/Centre_de_la_France # 543,7 km ;  Centre du cercle minimum \n",
    "Not convinced by the effect of the distance specification.\n",
    "\n",
    "* `tw.Near = \"Paris\"`\n",
    "Seems work pretty well. But introduce a bias. \"Paris\" is one of the tweet's topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `CRITICAL:root:twint.run:Twint:Feed:noDataExpecting value: line 1 column 1 (char 0)`\n",
    "\n",
    "https://github.com/twintproject/twint/issues/1063\n",
    "\n",
    "```\n",
    "sudo pip3 uninstall twint\n",
    "sudo pip3 install --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `RuntimeError: This event loop is already running`\n",
    "\n",
    "Solve compatibility issues with notebooks and RunTime errors.\n",
    "\n",
    "```\n",
    "sudo pip3 install nest_asyncio\n",
    "```\n",
    "\n",
    "Then in python code:\n",
    "```\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `CRITICAL:root:twint.run:Twint:Feed:noData'globalObjects'`\n",
    "\n",
    "When too many requests with no pause between them. A pause can be created simply by increasing the limit of the tweets to fetch. \n",
    "\n",
    "Approximatively we note that a limit set to 1000 last 10 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching some tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package\n",
    "import twint\n",
    "import nest_asyncio\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import calendar\n",
    "import os\n",
    "\n",
    "def add_time_delta(a_date):\n",
    "    #a_date += datetime.timedelta(minutes=15)\n",
    "    #a_date += datetime.timedelta(days=1)\n",
    "    a_date += datetime.timedelta(weeks=1)\n",
    "    return a_date\n",
    "#\n",
    "def add_months(sourcedate, months):\n",
    "    month = sourcedate.month - 1 + months\n",
    "    year = sourcedate.year + month // 12\n",
    "    month = month % 12 + 1\n",
    "    day = min(sourcedate.day, calendar.monthrange(year,month)[1])\n",
    "    return datetime.datetime(year, month, day,sourcedate.hour, sourcedate.minute, sourcedate.second)\n",
    "\n",
    "# emotions\n",
    "anger = \"😣 OR 😩 OR 😫 OR 💔 OR 😞 OR ‼ OR 👊 OR 🔫 OR 💥 OR ❗ OR 🔥 OR 🙅 OR 👎 OR ❌ OR 😒 OR 😤 OR 😡 OR 😠\"\n",
    "anticipation = \"🎀 OR 😉 OR ✌ OR 😻 OR 😊 OR ☺ OR 😜 OR 😕 OR 😱 OR ‼ OR ❗ OR 🍻 OR 😚 OR ✈ OR 🙏 OR 🙊 OR 😅 OR 😈 OR 😓 OR 😟 OR 😋 OR 😙 OR 💓 OR ✊ OR 🙋 OR 😬 OR 💰 OR 💭 OR 👀\"\n",
    "disgust = \"💔 OR 😶 OR 😨 OR 😳 OR 😭 OR 😢 OR 😕 OR 😰 OR 😱 OR 😪 OR 😟 OR 😑 OR 😞 OR 😩 OR 😷 OR 🙅 OR 😡 OR 😠 OR ❌ OR 😤 OR 😫 OR 😒 OR 💩 OR 😣 OR 😖 OR 👎\"\n",
    "fear = \"😞 OR 😭 OR 😥 OR 🙅 OR 😶 OR 😕 OR 👀 OR ❗ OR 😬 OR 😣 OR 😩 OR 🙊 OR 😷 OR 😢 OR 😖 OR 😳 OR 💀 OR 😓 OR 👻 OR 😟 OR 😰 OR 😱 OR 😨\"\n",
    "joy = \"💘 OR 💃 OR 😛 OR 😀 OR 🌈 OR 💐 OR ❤ OR 🙌 OR 😇 OR 🍻 OR ♥ OR 😘 OR 😎 OR 😻 OR 😋 OR 🌞 OR 💋 OR 😌 OR 😝 OR 😃 OR 😍 OR 💞 OR 💕 OR 😙 OR 😄 OR 😚 OR 😁 OR 💗 OR 💖 OR 🎉 OR 😊 OR 😂 OR 😹 OR 😆 OR ☺\"\n",
    "sadeness = \"😓 OR 😡 OR 😕 OR 😒 OR 😖 OR 😨 OR 😪 OR 😣 OR 😰 OR 😟 OR 😔 OR 😫 OR 😩 OR 😥 OR 😞 OR 💔 OR 😭 OR 😢\" # 15 minutes sur Paris\n",
    "surprise = \"⚡ OR 💐 OR 🎀 OR 😅 OR 🔥 OR 🙌 OR 😨 OR ☺ OR 🙊 OR ✨ OR 😳 OR 😍 OR 🎉 OR 🙈 OR 👀 OR 😱 OR ❗ OR ‼\"\n",
    "trust = \"😊 OR 🙌 OR ☑ OR ✌ OR 👍 OR 😉 OR 💎 OR 💗 OR 😇 OR 😍 OR ☺ OR 🍻 OR 💖 OR 👏 OR 💯 OR 👌 OR ♥ OR 💐 OR 🙏 OR 💞 OR 😌 OR 😘 OR 🙆 OR 👊 OR 💘 OR 💓 OR 😚 OR ❤ OR 💋 OR 🌹 OR 💕 OR 😙\"\n",
    "emotional_emojis_dict = dict()\n",
    "emotional_emojis_dict['anger'] = anger\n",
    "emotional_emojis_dict['anticipation'] = anticipation\n",
    "emotional_emojis_dict['disgust'] = disgust\n",
    "emotional_emojis_dict['fear'] = fear\n",
    "emotional_emojis_dict['joy'] = joy\n",
    "emotional_emojis_dict['sadeness'] = sadeness\n",
    "emotional_emojis_dict['surprise'] = surprise\n",
    "emotional_emojis_dict['trust'] = trust\n",
    "\n",
    "# twint\n",
    "nest_asyncio.apply()\n",
    "\n",
    "tw = twint.Config()\n",
    "tw.Pandas = True\n",
    "tw.Store_pandas = True\n",
    "tw.Pandas_clean = True\n",
    "tw.Hide_output = True\n",
    "\n",
    "\n",
    "plus_grandes_villes_de_france = ['Paris', 'Marseille', 'Lyon', 'Toulouse', 'Nice', 'Nantes', 'Montpellier', 'Strasbourg', 'Bordeaux', 'Lille', 'Rennes', 'Reims', 'Saint-Étienne', 'Le Havre', 'Toulon', 'Grenoble', 'Dijon', 'Angers', 'Nîmes', 'Villeurbanne']\n",
    "# 'Monaco', 'Bruxelles', 'Québec',  'London', 'Dublin', 'Madrid', 'Lisbone', 'Berlin', 'Rome', 'Vienne', 'Luxembourg', \n",
    "len(plus_grandes_villes_de_france)\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "\n",
    "# emotional_tweets_dict = dict()\n",
    "\n",
    "# for each time delta\n",
    "def fetch_tweets(start_date, end_date, add_time_delta, plus_grandes_villes_de_france, outputDir):\n",
    "    next_start_date = start_date\n",
    "    \n",
    "    tmp_start_date = start_date\n",
    "    tmp_next_start_date = next_start_date\n",
    "    total_delta_counter = 0\n",
    "    while tmp_start_date < end_date:\n",
    "        tmp_next_start_date = add_time_delta(tmp_next_start_date)\n",
    "        tmp_start_date = tmp_next_start_date\n",
    "        total_delta_counter += 1\n",
    "    print (\"INFO: time delta factor = \"+str(total_delta_counter))\n",
    "\n",
    "    \n",
    "    print (\"INFO: start_date: \"+str(start_date))\n",
    "    \n",
    "    scraping_start = time.time()\n",
    "    print (\"INFO: scraping Twitter starting at: \"+str(scraping_start)+\"...\")\n",
    "    \n",
    "    delta_counter = 0\n",
    "    while start_date < end_date:\n",
    "        next_start_date = add_time_delta(next_start_date)\n",
    "    \n",
    "        day_start = time.time()\n",
    "\n",
    "        # for each top city\n",
    "        city_counter = 0\n",
    "        # SET \n",
    "        #for v in plus_grandes_villes_de_france:\n",
    "        for v in plus_grandes_villes_de_france:\n",
    "            city_counter += 1\n",
    "            tw.Near = v\n",
    "            city_start = time.time()\n",
    "\n",
    "            # for each affect\n",
    "            emotion_counter = 0\n",
    "            for e in emotional_emojis_dict:\n",
    "                #print ('Debug: e\\t',e)\n",
    "                emotion_counter += 1\n",
    "                affect_start = time.time()\n",
    "\n",
    "                tw.Search =  emotional_emojis_dict[e]\n",
    "\n",
    "                #\n",
    "                tw.Since = str(start_date)\n",
    "                tw.Until = str(next_start_date)\n",
    "            \n",
    "                #\n",
    "                twint.run.Search(tw)\n",
    "                #if e not in emotional_tweets_dict:\n",
    "                #    emotional_tweets_dict[e] = twint.storage.panda.Tweets_df\n",
    "                #else:\n",
    "                #    emotional_tweets_dict[e] = emotional_tweets_dict[e].append(twint.storage.panda.Tweets_df, ignore_index=True)\n",
    "                current_d_v_e_df = twint.storage.panda.Tweets_df\n",
    "                affect_end = time.time()\n",
    "                outputfile = outputDir+\"/tweets_\"+e+'_'+v+'_'+str(start_date).replace(' ','_')+'_'+str(next_start_date).replace(' ','_')+'.csv' #'+.replace(' ','_')\n",
    "                print (\"INFO: \"+str(delta_counter)+\"/\"+str(total_delta_counter)+\"\\t\"+str(start_date).replace(' ','_')+\"\\t\"+v+\"(\"+str(city_counter)+\"/\"+str(len(plus_grandes_villes_de_france))+\")\\t\"+e+\"(\"+str(emotion_counter)+\"/\"+str(len(emotional_emojis_dict))+\")\\t\"+str(affect_end - affect_start))\n",
    "                if not (current_d_v_e_df.empty):\n",
    "                    current_d_v_e_df['emotion'] = [e]*len(current_d_v_e_df)\n",
    "                    current_d_v_e_df['city'] = [v]*len(current_d_v_e_df)\n",
    "                    current_d_v_e_df['since'] = [tw.Since.split(' ')[0]]*len(current_d_v_e_df)\n",
    "                \n",
    "                #print(\"INFO: scraping affect \"+e+\" ellapsed time: \"+str(affect_end - affect_start))\n",
    "                    print (\"INFO: Printing \"+outputfile) #+\" ; ellapsed time: \"+str(affect_end - affect_start))\n",
    "                #emotional_tweets_dict[e]\n",
    "                    current_d_v_e_df.to_csv(outputfile, sep = '\\t', index=False) \n",
    "            #else: \n",
    "\n",
    "            city_end = time.time()\n",
    "        #print(\"INFO: scraping city \"+v+\" ellapsed time: \"+str(city_end - city_start))\n",
    "    \n",
    "        day_end = time.time()\n",
    "    #print(\"INFO: scraping dat \"+str(start_date)+\" ellapsed time: \"+str(day_end - day_start))\n",
    "    \n",
    "        start_date = next_start_date\n",
    "        delta_counter += 1\n",
    "        print ()\n",
    "            \n",
    "    scraping_end = time.time()\n",
    "    print(\"INFO: scraping ellapsed time: \"+str(scraping_end - scraping_start))\n",
    "       #tweets_df = pd.DataFrame()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME how the data_path was populated...\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Setting 0\n",
    "start_date = datetime.datetime(2018,1,1,0,0,0)\n",
    "end_date = datetime.datetime(2018,1,8,0,0,0)\n",
    "\n",
    "plus_grandes_villes_de_france = ['Paris', 'Marseille', 'Lyon', 'Toulouse', 'Nice', 'Nantes', 'Montpellier', 'Strasbourg', 'Bordeaux', 'Lille', 'Rennes', 'Reims', 'Saint-Étienne', 'Le Havre', 'Toulon', 'Grenoble', 'Dijon', 'Angers', 'Nîmes', 'Villeurbanne']\n",
    "\n",
    "def add_time_delta(a_date):\n",
    "    a_date += datetime.timedelta(days=1)\n",
    "    return a_date\n",
    "\n",
    "# \n",
    "outputDir = \"built/100tweets_top20-city-fr_8emo_20180101-0107\"\n",
    "outputDir = \"/tmp/100tweets_top20-city-fr_8emo_20180101-0107\"\n",
    "\n",
    "if not(os.path.isfile(outputDir+\".zip\")) and not(os.path.isdir(outputDir)):\n",
    "    os.mkdir(outputDir)\n",
    "\n",
    "tw.Limit = 100 # 5000 ~ 60 # 1000 ~10s (sans Géo)\n",
    "tw.Lang = \"fr\"\n",
    "\n",
    "# UNcomment to run\n",
    "fetch_tweets(start_date, end_date, add_time_delta, plus_grandes_villes_de_france, outputDir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison de l'option native de stockage de twing et celle ultérieure via dataframe\n",
    "\n",
    "Après observations des deux fichiers produits. Avec dataframe, on peut choisir le séparateur et rajouter des colonnes. Il n'y a pas de perte d'information dans aucun des deux cas. Le champ tweet est le même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "# twint\n",
    "nest_asyncio.apply()\n",
    "\n",
    "tw = twint.Config()\n",
    "tw.Pandas = True\n",
    "tw.Store_pandas = True\n",
    "tw.Pandas_clean = True\n",
    "tw.Hide_output = True\n",
    "\n",
    "c = \"Paris\"\n",
    "e = \"Café\"\n",
    "tw.Limit = 1000 # 1000 ~10s (sans Géo)\n",
    "tw.Since = str(datetime.datetime(2018,1,1,0,0,0))\n",
    "tw.Until = str(datetime.datetime(2018,1,10,0,0,0))\n",
    "tw.Near = c\n",
    "tw.Search = e\n",
    "tw.Store_pandas = True\n",
    "outputName = \"/tmp/tweets_\"+e+'_'+c+'_'+tw.Since.replace(' ','_')+'_'+tw.Until .replace(' ','_') #+'.csv' #'+\n",
    "tw.Output = outputName+\"_tw.csv\"        \n",
    "twint.run.Search(tw)\n",
    "current_d_v_e_df = twint.storage.panda.Tweets_df\n",
    "current_d_v_e_df['emotion'] = [e]*len(current_d_v_e_df)\n",
    "current_d_v_e_df['city'] = [c]*len(current_d_v_e_df)\n",
    "current_d_v_e_df['since'] = [tw.Since.split(' ')[0]]*len(current_d_v_e_df)\n",
    "print (len(current_d_v_e_df))\n",
    "# current_d_v_e_df.head()\n",
    "# current_d_v_e_df['tweet'][0]\n",
    "current_d_v_e_df.to_csv(outputName+\"_pd.csv\", sep = '\\t', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo pip3 install langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "for t in tweet:\n",
    "    print (t['tweet'])\n",
    "    #if detect(t['tweet']) == 'fr': \n",
    "    #    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
