{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation et racinisation de texte en français\n",
    "( _Lemmatization and stemming_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"avions voudrais non animaux yeux dors couvre\"\n",
    "text = \"Apple cherche a acheter une startup anglaise pour 1 milliard de dollard\"\n",
    "text = \"J'utilise mes connaissances. Et nous les appliquons.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"j'utilis\", 'me', 'connaiss', '.', 'et', 'nous', 'le', 'appliquon', '.']\n",
      "[\"j'\", 'utilis', 'me', 'connaiss', '.', 'et', 'nous', 'le', 'appliquon', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()\n",
    "words_to_stem = text.split(' ')\n",
    "stems = [stemmer.stem(w) for w in words_to_stem]\n",
    "print (stems)\n",
    "print ([stemmer.stem(t.text) for t in nlp(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install --user spacy\n",
    "#\"python3 -m spacy download fr_core_news_md\n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je', 'utilise', 'mon', 'connaissance', '.', 'et', 'nous', 'le', 'appliquer', '.']\n"
     ]
    }
   ],
   "source": [
    "print ([token.lemma_ for token in nlp(text)])\n",
    "#for d in doc:\n",
    "#    print(d.text, d.pos_, d._.melt_tagger, d._.lefff_lemma, d.tag_, d.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrenchLefffLemmatizer\n",
    "* Source https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer\n",
    "* Licence Apache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avion\n",
      "avion\n",
      "avoir\n",
      "[]\n",
      "[\"J'\", 'utilise', 'mes', 'connaissance', '.', 'Et', 'nous', 'les', 'appliquons', '.']\n"
     ]
    }
   ],
   "source": [
    "# pip3 install --user git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "french_lefff_lemmatizer = FrenchLefffLemmatizer()\n",
    "print(french_lefff_lemmatizer.lemmatize('avions'))\n",
    "print(french_lefff_lemmatizer.lemmatize('avions','n'))\n",
    "print(french_lefff_lemmatizer.lemmatize('avions','v'))\n",
    "print(french_lefff_lemmatizer.lemmatize('avions','unk'))\n",
    "\n",
    "def french_lefff_lemmatizer_context_free (french_lefff_lemmatizer, tokenized_text):\n",
    "    return [french_lefff_lemmatizer.lemmatize(t) for t in tokenized_text]\n",
    "\n",
    "print(french_lefff_lemmatizer_context_free(french_lefff_lemmatizer, [t.text for t in nlp(text)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spacy tags to desambiguate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je', 'utilise', 'mon', 'connaissance', '.', 'et', 'nous', 'le', 'appliquer', '.']\n"
     ]
    }
   ],
   "source": [
    "#numbers_powers = list(map(pow, base_numbers, powers))\n",
    "#mapped_numbers = list(map(lambda x: x , numbers))\n",
    "spacy_to_lefff_pos = {\n",
    "    \"ADJ\": \"adj\",\n",
    "    \"ADP\": \"det\",\n",
    "    \"ADV\": \"adv\",\n",
    "    \"DET\": \"det\",\n",
    "    \"PRON\": \"cln\",\n",
    "    \"PROPN\": \"np\",\n",
    "    \"NOUN\": \"nc\",\n",
    "    \"VERB\": \"v\",\n",
    "    \"PUNCT\": \"poncts\"\n",
    "} # CCONJ ?\n",
    "\n",
    "\n",
    "def french_lefff_lemmatizer_wi_spacy_pos (french_lefff_lemmatizer, spacy_doc):\n",
    "    # lefff retourne de mauvais lemmes pour les DET et les PRON\n",
    "    # spacy retourne de mauvais lemmes pour les VERB\n",
    "    # retourne le lemme de spacy par défaut excepté pour les verbes\n",
    "    lemmas = []\n",
    "    for t in spacy_doc:\n",
    "        if t.pos_ in ['VERB']:\n",
    "            lefff_lemma = french_lefff_lemmatizer.lemmatize(t.text, spacy_to_lefff_pos[t.pos_])\n",
    "            if type(lefff_lemma) != type (\"\") and len(lefff_lemma) !=0:\n",
    "                lefff_lemma = lefff_lemma[0][0]\n",
    "            else: lefff_lemma = t.lemma_\n",
    "            lemmas.append(lefff_lemma)\n",
    "        else:\n",
    "            lemmas.append(t.lemma_)\n",
    "\n",
    "    return lemmas\n",
    "print (french_lefff_lemmatizer_wi_spacy_pos(french_lefff_lemmatizer, nlp(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy-lefff\n",
    "\n",
    "**Custom French POS and lemmatizer based on Lefff for spacy**\n",
    "\n",
    "* Source https://pypi.org/project/spacy-lefff/\n",
    "* License MIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Language' has no attribute 'factory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1c5dec78aca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'french_lemmatizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_french_lemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mLefffLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter_melt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Language' has no attribute 'factory'"
     ]
    }
   ],
   "source": [
    "# pip3 install --user spacy-lefff\n",
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.factory('french_lemmatizer')\n",
    "def create_french_lemmatizer(nlp, name):\n",
    "    return LefffLemmatizer(after_melt=True, default=True)\n",
    "\n",
    "@Language.factory('melt_tagger')  \n",
    "def create_melt_tagger(nlp, name):\n",
    "    return POSTagger()\n",
    " \n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "#nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "nlp.add_pipe('melt_tagger', after='parser')\n",
    "nlp.add_pipe('french_lemmatizer', after='melt_tagger')\n",
    "doc = nlp(u\"Apple cherche a acheter une startup anglaise pour 1 milliard de dollard\")\n",
    "for d in doc:\n",
    "    print(d.text, d.pos_, d._.melt_tagger, d._.lefff_lemma, d.tag_, d.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## treetagger-python\n",
    "\n",
    "**A Python module for interfacing with the Treetagger by Helmut Schmid**\n",
    "\n",
    "* Wrapper source https://github.com/miotto/treetagger-python (alternative exists)\n",
    "* License GPL-v3\n",
    "* TreeTagger source https://www.cis.lmu.de/~schmid/tools/TreeTagger/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLTK, The Classical Language Toolkit \n",
    "\n",
    "**The Classical Language Toolkit (CLTK) is a Python library offering natural language processing (NLP) for pre-modern languages.**\n",
    "\n",
    "* Home https://github.com/cltk/cltk \n",
    "* Licence MIT\n",
    "* Doc https://docs.cltk.org/en/latest/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NLP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-49e598964ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pip3 install cltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#  Middle French language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcltk_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mfr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'NLP'"
     ]
    }
   ],
   "source": [
    "# pip3 install --user cltk\n",
    "from cltk import NLP\n",
    "\n",
    "#  Middle French language\n",
    "cltk_nlp = NLP(language=\"mfr\")\n",
    "cltk_doc = cltk_nlp.analyze(text=text)\n",
    "print(cltk_doc.lemmata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LemmaReplace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-23ea0a45f19c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLemmaReplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LemmaReplace'"
     ]
    }
   ],
   "source": [
    "from cltk.lemmatize.french.lemma import LemmaReplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "* spacy-lefff que je n'ai pu tester, probablement pour des questions de configurations d'environnements de jupyter\n",
    "* treetagger-python qui requiert de pré-installer treetagger\n",
    "* malgré une installation pip sans erreur, problème à l'exécution de cltk ; dans tous les cas n'était pas pour du français contemporain \n",
    "* spacy pour la robustesse, spacy+lefff pour la précision  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
