{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation et racinisation de texte en français\n",
    "( _Lemmatization and stemming_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"avions voudrais non animaux yeux dors couvre\"\n",
    "text = \"Apple cherche a acheter une startup anglaise pour 1 milliard de dollard\"\n",
    "text = \"J'utilise mes connaissances. Et nous les appliquons.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"j'utilis\", 'me', 'connaiss', '.', 'et', 'nous', 'le', 'appliquon', '.']\n"
     ]
    }
   ],
   "source": [
    "# sudo pip3 install nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print([FrenchStemmer().stem(w) for w in word_tokenize(text, language='french')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tokenizations issues\n",
    "* stemming is what it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy\n",
    "* https://spacy.io/models (two models for French, one more efficient and the other more accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je', 'utilise', 'mon', 'connaissance', '.', 'et', 'nous', 'le', 'appliquon', '.']\n"
     ]
    }
   ],
   "source": [
    "# sudo pip3 install spacy\n",
    "# sudo python3 -m spacy download fr_core_news_sm \n",
    "# sudo python3 -m spacy download fr_dep_news_trf \n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm') # efficiency\n",
    "\n",
    "# what spacy can produce (tokenization, pos tagging, lemmatization)\n",
    "#for w in nlp(text):\n",
    "#    print(w.text, w.pos_, w.tag_, w.lemma_)\n",
    "# spacy lemmatization\n",
    "\n",
    "print ([token.lemma_ for token in nlp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je', 'utilise', 'mon', 'connaissance', '.', 'et', 'nous', 'le', 'appliquer', '.']\n"
     ]
    }
   ],
   "source": [
    "# spacy lemmatization\n",
    "nlp = spacy.load('fr_dep_news_trf') # accuracy\n",
    "print ([token.lemma_ for token in nlp(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lemmatization issues mainly for verbs\n",
    "* a model more accurate than another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrenchLefffLemmatizer\n",
    "* Source https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer\n",
    "* Licence Apache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avion\n",
      "avion\n",
      "avoir\n",
      "[]\n",
      "utilise\n",
      "utiliser\n",
      "appliquons\n",
      "appliquer\n",
      "spacy_text\tspacy_pos\tspacy_lemma\tlefff_pos\tlefff_lemma\tlefff+pos_lemma\n",
      "J'\t\tPRON\t\tje\t\tcln\t\tj'\t\t[('il', 'cln')]\n",
      "utilise\t\tVERB\t\tutilise\t\tv\t\tutilise\t\tutiliser\n",
      "mes\t\tDET\t\tmon\t\tdet\t\tmes\t\t[('son', 'det')]\n",
      "connaissances\t\tNOUN\t\tconnaissance\t\tnc\t\tconnaissance\t\t[('connaissance', 'nc')]\n",
      ".\t\tPUNCT\t\t.\t\tponcts\t\t.\t\t[('.', 'poncts')]\n",
      "Et\t\tCCONJ\t\tet\t\tCCONJ\t\tet\t\t[]\n",
      "nous\t\tPRON\t\tnous\t\tcln\t\tnous\t\t[('il', 'cln')]\n",
      "les\t\tPRON\t\tle\t\tcln\t\tles\t\t[]\n",
      "appliquons\t\tVERB\t\tappliquer\t\tv\t\tappliquons\t\tappliquer\n",
      ".\t\tPUNCT\t\t.\t\tponcts\t\t.\t\t[('.', 'poncts')]\n",
      "[\"J'\", 'utilise', 'mes', 'connaissance', '.', 'Et', 'nous', 'les', 'appliquons', '.']\n",
      "[\"j'\", 'utilise', 'mes', 'connaissance', '.', 'et', 'nous', 'les', 'appliquons', '.']\n",
      "['je', 'utilise', 'mon', 'connaissance', '.', 'et', 'nous', 'le', 'appliquer', '.']\n"
     ]
    }
   ],
   "source": [
    "# sudo pip3 install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "french_lefff_lemmatizer = FrenchLefffLemmatizer()\n",
    "\n",
    "print(french_lefff_lemmatizer.lemmatize('avions'))\n",
    "print(french_lefff_lemmatizer.lemmatize('avions','n'))\n",
    "print(french_lefff_lemmatizer.lemmatize('avions','v'))\n",
    "print(french_lefff_lemmatizer.lemmatize('avions','pipo'))\n",
    "print(french_lefff_lemmatizer.lemmatize('utilise'))\n",
    "print(french_lefff_lemmatizer.lemmatize('utilise','v'))\n",
    "print(french_lefff_lemmatizer.lemmatize('appliquons'))\n",
    "print(french_lefff_lemmatizer.lemmatize('appliquons','v'))\n",
    "\n",
    "def french_lefff_lemmatizer_context_free (french_lefff_lemmatizer, tokenized_text):\n",
    "    return [french_lefff_lemmatizer.lemmatize(t) for t in tokenized_text]\n",
    "\n",
    "print ('\\t'.join(['spacy_text', 'spacy_pos', 'spacy_lemma', 'lefff_pos', 'lefff_lemma', 'lefff+pos_lemma']))\n",
    "for t in nlp(text):\n",
    "    print ('\\t\\t'.join([t.text,t.pos_,t.lemma_, spacy_to_lefff_pos[t.pos_] if t.pos_ in spacy_to_lefff_pos else t.pos_, french_lefff_lemmatizer.lemmatize(t.text.lower()), str(french_lefff_lemmatizer.lemmatize(t.text,spacy_to_lefff_pos[t.pos_] if t.pos_ in spacy_to_lefff_pos else ''))]))\n",
    "        \n",
    "print(french_lefff_lemmatizer_context_free(french_lefff_lemmatizer, [t.text for t in nlp(text)]))\n",
    "print(french_lefff_lemmatizer_context_free(french_lefff_lemmatizer, [t.text.lower() for t in nlp(text)]))\n",
    "print(french_lefff_lemmatizer_context_free(french_lefff_lemmatizer, [t.lemma_ for t in nlp(text)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FrenchLefffLemmatizer can use pos tag informations to desambiguate \n",
    "* FrenchLefffLemmatizer can perform better than spacy on verb lemmatization with pos tag; do some errors on pronouns and determiners\n",
    "* FrenchLefffLemmatizer return either a string or a list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spacy tags to desambiguate - home made solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"J'\", 'PRON', 'je', 'cln', \"j'\", [('il', 'cln')]]\n",
      "['utilise', 'VERB', 'utilise', 'v', 'utilise', 'utiliser']\n",
      "to fix\n",
      "is a string so lemma is : utiliser\n",
      "['mes', 'DET', 'mon', 'det', 'mes', [('son', 'det')]]\n",
      "['connaissances', 'NOUN', 'connaissance', 'nc', 'connaissance', [('connaissance', 'nc')]]\n",
      "['.', 'PUNCT', '.', 'poncts', '.', [('.', 'poncts')]]\n",
      "['Et', 'CCONJ', 'et', 'CCONJ', 'et', []]\n",
      "['nous', 'PRON', 'nous', 'cln', 'nous', [('il', 'cln')]]\n",
      "['les', 'PRON', 'le', 'cln', 'les', []]\n",
      "['appliquons', 'VERB', 'appliquer', 'v', 'appliquons', 'appliquer']\n",
      "to fix\n",
      "is a string so lemma is : appliquer\n",
      "['.', 'PUNCT', '.', 'poncts', '.', [('.', 'poncts')]]\n",
      "['je', 'utiliser', 'mon', 'connaissance', '.', 'et', 'nous', 'le', 'appliquer', '.']\n"
     ]
    }
   ],
   "source": [
    "spacy_to_lefff_pos = {\n",
    "    \"ADJ\": \"adj\",\n",
    "    \"ADP\": \"det\",\n",
    "    \"ADV\": \"adv\",\n",
    "    \"DET\": \"det\",\n",
    "    \"PRON\": \"cln\",\n",
    "    \"PROPN\": \"np\",\n",
    "    \"NOUN\": \"nc\",\n",
    "    \"VERB\": \"v\",\n",
    "    \"PUNCT\": \"poncts\"\n",
    "} # CCONJ ?\n",
    "\n",
    "\n",
    "def french_lefff_lemmatizer_context_sensitive_wi_spacy_pos (french_lefff_lemmatizer, spacy_doc):\n",
    "    ''' \n",
    "    Given that lefff returns wrong lemma for DET and PRON\n",
    "    Given that spacy returns wrong lemma for VERB\n",
    "    Returns the spacy lemma except for VERB\n",
    "    '''\n",
    "    lemmas = []\n",
    "    for t in spacy_doc:\n",
    "        #print ([t.text,t.pos_,t.lemma_, spacy_to_lefff_pos[t.pos_] if t.pos_ in spacy_to_lefff_pos else t.pos_, french_lefff_lemmatizer.lemmatize(t.text.lower()), french_lefff_lemmatizer.lemmatize(t.text,spacy_to_lefff_pos[t.pos_] if t.pos_ in spacy_to_lefff_pos else '')])\n",
    "        if t.pos_ in ['VERB']:\n",
    "            #print ('to fix')\n",
    "            lefff_lemma = french_lefff_lemmatizer.lemmatize(t.text, spacy_to_lefff_pos[t.pos_])\n",
    "            if type(lefff_lemma) != type (\"\") and len(lefff_lemma) !=0:\n",
    "                lefff_lemma = lefff_lemma[0][0]\n",
    "                #print ('is a list so lemma is : '+lefff_lemma)\n",
    "            else: \n",
    "                lefff_lemma =lefff_lemma\n",
    "                #print ('is a string so lemma is : '+lefff_lemma)\n",
    "            lemmas.append(lefff_lemma)\n",
    "        else:\n",
    "            lemmas.append(t.lemma_)\n",
    "\n",
    "    return lemmas\n",
    "print (french_lefff_lemmatizer_context_sensitive_wi_spacy_pos(french_lefff_lemmatizer, nlp(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* on our example, the lemmatization performs well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy-lefff\n",
    "\n",
    "**Custom French POS and lemmatizer based on Lefff for spacy**\n",
    "\n",
    "* Source https://pypi.org/project/spacy-lefff/\n",
    "* License MIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/usr/local/lib/python3.6/dist-packages/spacy_lefff/data/tagger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2b76b1d8959d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fr_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# efficiency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'melt_tagger'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'french_lemmatizer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'melt_tagger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu\"Apple cherche a acheter une startup anglaise pour 1 milliard de dollard\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    771\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                 \u001b[0mraw_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                 \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m             )\n\u001b[1;32m    775\u001b[0m         \u001b[0mpipe_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pipe_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mcreate_pipe\u001b[0;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# We're calling the internal _fill here to avoid constructing the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;31m# registered functions twice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0mresolved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m         \u001b[0mfilled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"cfg\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cfg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mfilled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(cls, config, schema, overrides, validate)\u001b[0m\n\u001b[1;32m    721\u001b[0m     ) -> Dict[str, Any]:\n\u001b[1;32m    722\u001b[0m         resolved, _ = cls._make(\n\u001b[0;32m--> 723\u001b[0;31m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         )\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresolved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36m_make\u001b[0;34m(cls, config, schema, overrides, resolve, validate)\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         filled, _, resolved = cls._fill(\n\u001b[0;32m--> 772\u001b[0;31m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m         )\n\u001b[1;32m    774\u001b[0m         \u001b[0mfilled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msection_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msection_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36m_fill\u001b[0;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[1;32m    841\u001b[0m                     \u001b[0;31m# We don't want to try/except this and raise our own error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                     \u001b[0;31m# here, because we want the traceback if the function fails.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                     \u001b[0mgetter_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0;31m# We're not resolving and calling the function, so replace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b76b1d8959d>\u001b[0m in \u001b[0;36mcreate_melt_tagger\u001b[0;34m(nlp, name)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'melt_tagger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_melt_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPOSTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fr_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# efficiency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy_lefff/melt_tagger.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, model_dir_path, lexicon_file_path, tag_file_path, package, url, print_probas)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             download_dir=data_dir)\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy_lefff/downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pkg, url, download_dir)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpkg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/usr/local/lib/python3.6/dist-packages/spacy_lefff/data/tagger'"
     ]
    }
   ],
   "source": [
    "# sudo pip3 install spacy-lefff\n",
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.factory('french_lemmatizer')\n",
    "def create_french_lemmatizer(nlp, name):\n",
    "    return LefffLemmatizer(after_melt=True, default=True)\n",
    "\n",
    "@Language.factory('melt_tagger')  \n",
    "def create_melt_tagger(nlp, name):\n",
    "    return POSTagger()\n",
    " \n",
    "nlp = spacy.load('fr_core_news_sm') # efficiency\n",
    "\n",
    "nlp.add_pipe('melt_tagger', after='parser')\n",
    "nlp.add_pipe('french_lemmatizer', after='melt_tagger')\n",
    "doc = nlp(u\"Apple cherche a acheter une startup anglaise pour 1 milliard de dollard\")\n",
    "for d in doc:\n",
    "    print(d.text, d.pos_, d._.melt_tagger, d._.lefff_lemma, d.tag_, d.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `PermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.6/dist-packages/spacy_lefff/data/tagger'` ; the path `/usr/local/lib/python3.6/dist-packages/spacy_lefff/data/'` exists but no tagger in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stanza aka Neural Stanford corenlp\n",
    "\n",
    "Aside from the neural pipeline, this package also includes an official wrapper for accessing the Java Stanford CoreNLP software with Python code https://github.com/stanfordnlp/CoreNLP.\n",
    "\n",
    "* 800 Mo of parameters + 600 for the French model (stored in ~/stanza_resources/fr/default.zip)\n",
    "\n",
    "* https://github.com/stanfordnlp/stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 20.3MB/s]                    \n",
      "2021-04-14 11:59:15 INFO: Downloading default packages for language: fr (French)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/fr/default.zip: 100%|██████████| 574M/574M [02:05<00:00, 4.59MB/s] \n",
      "2021-04-14 12:01:26 INFO: Finished downloading models and saved to /home/hernandez-n/stanza_resources.\n",
      "2021-04-14 12:01:26 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| ner       | wikiner |\n",
      "=======================\n",
      "\n",
      "2021-04-14 12:01:26 INFO: Use device: cpu\n",
      "2021-04-14 12:01:26 INFO: Loading: tokenize\n",
      "2021-04-14 12:01:26 INFO: Loading: mwt\n",
      "2021-04-14 12:01:26 INFO: Loading: pos\n",
      "2021-04-14 12:01:27 INFO: Loading: lemma\n",
      "2021-04-14 12:01:27 INFO: Loading: depparse\n",
      "2021-04-14 12:01:27 INFO: Loading: ner\n",
      "2021-04-14 12:01:28 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"J'\", 2, 'nsubj')\n",
      "('utilise', 0, 'root')\n",
      "('mes', 4, 'det')\n",
      "('connaissances', 2, 'obj')\n",
      "('.', 2, 'punct')\n",
      "J' il PRON\n",
      "utilise utiliser VERB\n",
      "mes son DET\n",
      "connaissances connaissance NOUN\n",
      ". . PUNCT\n",
      "Et et CCONJ\n",
      "nous il PRON\n",
      "les le PRON\n",
      "appliquons appliquer VERB\n",
      ". . PUNCT\n"
     ]
    }
   ],
   "source": [
    "# sudo pip3 install stanza \n",
    "import stanza\n",
    "stanza.download('fr')       # This downloads the English models for the neural pipeline\n",
    "nlp = stanza.Pipeline('fr') # This sets up a default neural pipeline in English\n",
    "# doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "doc.sentences[0].print_dependencies()\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(word.text, word.lemma, word.pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* correct lemmatization on VERBs ; incorrect lemmatization for PRONouns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## treetagger-python\n",
    "\n",
    "**A Python module for interfacing with the Treetagger by Helmut Schmid**\n",
    "\n",
    "* Wrapper source https://github.com/miotto/treetagger-python (alternative exists)\n",
    "* License GPL-v3\n",
    "* TreeTagger source https://www.cis.lmu.de/~schmid/tools/TreeTagger/\n",
    "* A neural version exists too. It lemmatizes all tokens. Lemmas of unknown tokens are guessed and are therefore not guaranteed to be always correct. Slower, requires PyTorch, requires a GPU for improved speed, larger parameter files https://www.cis.lmu.de/~schmid/tools/RNNTagger/  (3.1 GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLTK, The Classical Language Toolkit \n",
    "\n",
    "**The Classical Language Toolkit (CLTK) is a Python library offering natural language processing (NLP) for pre-modern languages.**\n",
    "\n",
    "* Home https://github.com/cltk/cltk \n",
    "* Licence MIT\n",
    "* Doc https://docs.cltk.org/en/latest/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NLP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-49e598964ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pip3 install cltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#  Middle French language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcltk_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mfr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'NLP'"
     ]
    }
   ],
   "source": [
    "# pip3 install --user cltk\n",
    "from cltk import NLP\n",
    "\n",
    "#  Middle French language\n",
    "cltk_nlp = NLP(language=\"mfr\")\n",
    "cltk_doc = cltk_nlp.analyze(text=text)\n",
    "print(cltk_doc.lemmata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LemmaReplace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-23ea0a45f19c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLemmaReplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LemmaReplace'"
     ]
    }
   ],
   "source": [
    "from cltk.lemmatize.french.lemma import LemmaReplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## textblob-fr\n",
    "\n",
    "does not offer Words Inflection and Lemmatization for French\n",
    "\n",
    "https://pypi.org/project/textblob-fr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    ">>> from textblob import Word\n",
    ">>> w = Word(\"octopi\")\n",
    ">>> w.lemmatize()\n",
    "'octopus'\n",
    ">>> w = Word(\"went\")\n",
    ">>> w.lemmatize(\"v\")  # Pass in WordNet part of speech (verb)\n",
    "\n",
    "from textblob import TextBlob\n",
    ">>> from textblob_fr import PatternTagger, PatternAnalyzer\n",
    ">>> text = u\"Quelle belle matinée\"\n",
    ">>> blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    ">>> blob.tags\n",
    "[(u'Quelle', u'DT'), (u'belle', u'JJ'), (u'matin\\xe9e', u'NN')]\n",
    ">>> blob.sentiment\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pattern\n",
    "2.6 \n",
    "https://github.com/clips/pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "En conclusion du tour d'horizon des solutions de lemmatisation en python 3 pour le traitement du français contemporain\n",
    "\n",
    "* stanza, et textblob-fr ne fournissent pas de lemmatization. \n",
    "* pattern 2.6 n'est pas encore compatible python 3\n",
    "* je n'ai pu tester spacy-lefff, probablement pour des questions de configurations d'environnements de jupyter\n",
    "* je n'ai pas pu tester non plus cltk. Malgré une installation pip sans erreur, problème à l'exécution ; dans tous les cas n'était pas pour du français contemporain \n",
    "* treetagger-python mis de côté car requiert de pré-installer treetagger ; \n",
    "* n'ai pas testé rnntagger le nouvel outil en python de l'auteur de treetagger car celui-ci fait plus de 3 Go\n",
    "\" N'ai pas testé le stanford corenlp (java) pour lequel stanza offre un wrapper et qui fait de la lemmatization https://github.com/stanfordnlp/CoreNLP\n",
    "\n",
    "* spacy pour la robustesse, spacy+lefff pour la précision  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
