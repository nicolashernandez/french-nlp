{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation et racinisation de texte en français\n",
    "( _Lemmatization and stemming_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"avions voudrais non animaux yeux dors couvre\"\n",
    "text = \"Apple cherche a acheter une startup anglaise pour 1 milliard de dollard\"\n",
    "text = \"J'utilise mes connaissances. Et nous les appliquons.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"j'utilis\", 'me', 'connaiss', '.', 'et', 'nous', 'le', 'appliquon', '.']\n",
      "[\"j'\", 'utilis', 'me', 'connaiss', '.', 'et', 'nous', 'le', 'appliquon', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()\n",
    "words_to_stem = text.split(' ')\n",
    "stems = [stemmer.stem(w) for w in words_to_stem]\n",
    "print (stems)\n",
    "print ([stemmer.stem(t.text) for t in nlp(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install --user spacy\n",
    "#\"python3 -m spacy download fr_core_news_md\n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je', 'utilise', 'mon', 'connaissance', '.', 'et', 'nous', 'le', 'appliquer', '.']\n"
     ]
    }
   ],
   "source": [
    "print ([token.lemma_ for token in nlp(text)])\n",
    "#for d in doc:\n",
    "#    print(d.text, d.pos_, d._.melt_tagger, d._.lefff_lemma, d.tag_, d.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrenchLefffLemmatizer\n",
    "* Source https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer\n",
    "* Licence Apache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avion\n",
      "avion\n",
      "avoir\n",
      "[]\n",
      "[\"J'\", 'utilise', 'mes', 'connaissance', '.', 'Et', 'nous', 'les', 'appliquons', '.']\n"
     ]
    }
   ],
   "source": [
    "# pip3 install --user git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "french_lefff_lemmatizer = FrenchLefffLemmatizer()\n",
    "print(french_lefff_lemmatizer.lemmatize('avions'))\n",
    "print(french_lefff_lemmatizer.lemmatize('avions','n'))\n",
    "print(french_lefff_lemmatizer.lemmatize('avions','v'))\n",
    "print(french_lefff_lemmatizer.lemmatize('avions','unk'))\n",
    "\n",
    "def french_lefff_lemmatizer_context_free (french_lefff_lemmatizer, tokenized_text):\n",
    "    return [french_lefff_lemmatizer.lemmatize(t) for t in tokenized_text]\n",
    "\n",
    "print(french_lefff_lemmatizer_context_free(french_lefff_lemmatizer, [t.text for t in nlp(text)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spacy tags to desambiguate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je', 'utilise', 'mon', 'connaissance', '.', 'et', 'nous', 'le', 'appliquer', '.']\n"
     ]
    }
   ],
   "source": [
    "#numbers_powers = list(map(pow, base_numbers, powers))\n",
    "#mapped_numbers = list(map(lambda x: x , numbers))\n",
    "spacy_to_lefff_pos = {\n",
    "    \"ADJ\": \"adj\",\n",
    "    \"ADP\": \"det\",\n",
    "    \"ADV\": \"adv\",\n",
    "    \"DET\": \"det\",\n",
    "    \"PRON\": \"cln\",\n",
    "    \"PROPN\": \"np\",\n",
    "    \"NOUN\": \"nc\",\n",
    "    \"VERB\": \"v\",\n",
    "    \"PUNCT\": \"poncts\"\n",
    "} # CCONJ ?\n",
    "\n",
    "\n",
    "def french_lefff_lemmatizer_wi_spacy_pos (french_lefff_lemmatizer, spacy_doc):\n",
    "    # lefff retourne de mauvais lemmes pour les DET et les PRON\n",
    "    # spacy retourne de mauvais lemmes pour les VERB\n",
    "    # retourne le lemme de spacy par défaut excepté pour les verbes\n",
    "    lemmas = []\n",
    "    for t in spacy_doc:\n",
    "        if t.pos_ in ['VERB']:\n",
    "            lefff_lemma = french_lefff_lemmatizer.lemmatize(t.text, spacy_to_lefff_pos[t.pos_])\n",
    "            if type(lefff_lemma) != type (\"\") and len(lefff_lemma) !=0:\n",
    "                lefff_lemma = lefff_lemma[0][0]\n",
    "            else: lefff_lemma = t.lemma_\n",
    "            lemmas.append(lefff_lemma)\n",
    "        else:\n",
    "            lemmas.append(t.lemma_)\n",
    "\n",
    "    return lemmas\n",
    "print (french_lefff_lemmatizer_wi_spacy_pos(french_lefff_lemmatizer, nlp(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy-lefff\n",
    "\n",
    "**Custom French POS and lemmatizer based on Lefff for spacy**\n",
    "\n",
    "* Source https://pypi.org/project/spacy-lefff/\n",
    "* License MIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Language' has no attribute 'factory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1c5dec78aca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'french_lemmatizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_french_lemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mLefffLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter_melt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Language' has no attribute 'factory'"
     ]
    }
   ],
   "source": [
    "# pip3 install --user spacy-lefff\n",
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.factory('french_lemmatizer')\n",
    "def create_french_lemmatizer(nlp, name):\n",
    "    return LefffLemmatizer(after_melt=True, default=True)\n",
    "\n",
    "@Language.factory('melt_tagger')  \n",
    "def create_melt_tagger(nlp, name):\n",
    "    return POSTagger()\n",
    " \n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "#nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "nlp.add_pipe('melt_tagger', after='parser')\n",
    "nlp.add_pipe('french_lemmatizer', after='melt_tagger')\n",
    "doc = nlp(u\"Apple cherche a acheter une startup anglaise pour 1 milliard de dollard\")\n",
    "for d in doc:\n",
    "    print(d.text, d.pos_, d._.melt_tagger, d._.lefff_lemma, d.tag_, d.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stanza aka Neural Stanford corenlp\n",
    "\n",
    "Aside from the neural pipeline, this package also includes an official wrapper for accessing the Java Stanford CoreNLP software with Python code https://github.com/stanfordnlp/CoreNLP.\n",
    "\n",
    "* 800 Mo of parameters + 600 for the French model (stored in ~/stanza_resources/fr/default.zip)\n",
    "\n",
    "* https://github.com/stanfordnlp/stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json:   0%|          | 0.00/22.5k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 15.7MB/s]                    \u001b[A2021-04-09 01:03:28 INFO: Downloading default packages for language: fr (French)...\n",
      "2021-04-09 01:03:29 INFO: File exists: /home/hernandez-n/stanza_resources/fr/default.zip.\n",
      "2021-04-09 01:03:35 INFO: Finished downloading models and saved to /home/hernandez-n/stanza_resources.\n",
      "2021-04-09 01:03:35 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| ner       | wikiner |\n",
      "=======================\n",
      "\n",
      "2021-04-09 01:03:35 INFO: Use device: cpu\n",
      "2021-04-09 01:03:35 INFO: Loading: tokenize\n",
      "2021-04-09 01:03:35 INFO: Loading: mwt\n",
      "2021-04-09 01:03:35 INFO: Loading: pos\n",
      "2021-04-09 01:03:36 INFO: Loading: lemma\n",
      "2021-04-09 01:03:36 INFO: Loading: depparse\n",
      "2021-04-09 01:03:36 INFO: Loading: ner\n",
      "2021-04-09 01:03:38 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Barack', 0, 'root')\n",
      "('Obama', 1, 'flat:foreign')\n",
      "('was', 2, 'flat:foreign')\n",
      "('born', 3, 'flat:foreign')\n",
      "('in', 4, 'flat:foreign')\n",
      "('Hawaii', 5, 'flat:foreign')\n",
      "('.', 1, 'punct')\n",
      "Barack Barack X\n",
      "Obama Obama X\n",
      "was was X\n",
      "born born X\n",
      "in in X\n",
      "Hawaii Hawaii X\n",
      ". . PUNCT\n",
      "He He X\n",
      "was was X\n",
      "elected elected X\n",
      "president president X\n",
      "in in X\n",
      "2008 2008 NUM\n",
      ". . PUNCT\n"
     ]
    }
   ],
   "source": [
    "# pip3 install --user stanza \n",
    "import stanza\n",
    "stanza.download('fr')       # This downloads the English models for the neural pipeline\n",
    "nlp = stanza.Pipeline('fr') # This sets up a default neural pipeline in English\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "doc.sentences[0].print_dependencies()\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(word.text, word.lemma, word.pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## treetagger-python\n",
    "\n",
    "**A Python module for interfacing with the Treetagger by Helmut Schmid**\n",
    "\n",
    "* Wrapper source https://github.com/miotto/treetagger-python (alternative exists)\n",
    "* License GPL-v3\n",
    "* TreeTagger source https://www.cis.lmu.de/~schmid/tools/TreeTagger/\n",
    "* A neural version exists too. It lemmatizes all tokens. Lemmas of unknown tokens are guessed and are therefore not guaranteed to be always correct. Slower, requires PyTorch, requires a GPU for improved speed, larger parameter files https://www.cis.lmu.de/~schmid/tools/RNNTagger/  (3.1 GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLTK, The Classical Language Toolkit \n",
    "\n",
    "**The Classical Language Toolkit (CLTK) is a Python library offering natural language processing (NLP) for pre-modern languages.**\n",
    "\n",
    "* Home https://github.com/cltk/cltk \n",
    "* Licence MIT\n",
    "* Doc https://docs.cltk.org/en/latest/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NLP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-49e598964ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pip3 install cltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#  Middle French language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcltk_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mfr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'NLP'"
     ]
    }
   ],
   "source": [
    "# pip3 install --user cltk\n",
    "from cltk import NLP\n",
    "\n",
    "#  Middle French language\n",
    "cltk_nlp = NLP(language=\"mfr\")\n",
    "cltk_doc = cltk_nlp.analyze(text=text)\n",
    "print(cltk_doc.lemmata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LemmaReplace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-23ea0a45f19c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLemmaReplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LemmaReplace'"
     ]
    }
   ],
   "source": [
    "from cltk.lemmatize.french.lemma import LemmaReplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## textblob-fr\n",
    "\n",
    "does not offer Words Inflection and Lemmatization for French\n",
    "\n",
    "https://pypi.org/project/textblob-fr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    ">>> from textblob import Word\n",
    ">>> w = Word(\"octopi\")\n",
    ">>> w.lemmatize()\n",
    "'octopus'\n",
    ">>> w = Word(\"went\")\n",
    ">>> w.lemmatize(\"v\")  # Pass in WordNet part of speech (verb)\n",
    "\n",
    "from textblob import TextBlob\n",
    ">>> from textblob_fr import PatternTagger, PatternAnalyzer\n",
    ">>> text = u\"Quelle belle matinée\"\n",
    ">>> blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    ">>> blob.tags\n",
    "[(u'Quelle', u'DT'), (u'belle', u'JJ'), (u'matin\\xe9e', u'NN')]\n",
    ">>> blob.sentiment\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pattern\n",
    "2.6 \n",
    "https://github.com/clips/pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "En conclusion du tour d'horizon des solutions de lemmatisation en python 3 pour le traitement du français contemporain\n",
    "\n",
    "* stanza, et textblob-fr ne fournissent pas de lemmatization. \n",
    "* pattern 2.6 n'est pas encore compatible python 3\n",
    "* je n'ai pu tester spacy-lefff, probablement pour des questions de configurations d'environnements de jupyter\n",
    "* je n'ai pas pu tester non plus cltk. Malgré une installation pip sans erreur, problème à l'exécution ; dans tous les cas n'était pas pour du français contemporain \n",
    "* treetagger-python mis de côté car requiert de pré-installer treetagger ; \n",
    "* n'ai pas testé rnntagger le nouvel outil en python de l'auteur de treetagger car celui-ci fait plus de 3 Go\n",
    "\" N'ai pas testé le stanford corenlp (java) pour lequel stanza offre un wrapper et qui https://github.com/stanfordnlp/CoreNLP\n",
    "\n",
    "* spacy pour la robustesse, spacy+lefff pour la précision  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
